{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization (Senetnce Ranking)\n",
    "\n",
    "#### STEP 1 : Data cleaning ( removing non letter characters, turning to lower case letters )\n",
    "#### STEP 2 : Building Sentence Similarity Matrix\n",
    "#### STEP 3 : Sentence Ranking\n",
    "#### STEP 4 : Summary Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Phase\n",
    "### Importing Libraries and Reading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BASE_DIR = r\"C:\\Users\\LENOVO\\OneDrive\\Documents\\STKI\\Minggu 11\"\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2025.11.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2025.11.3-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   ---------------------------------------- 5/5 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.1 joblib-1.5.3 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas nltk networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing the necessary libraries\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from nltk.tokenize import  sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LENOVO\\\\OneDrive\\\\Documents\\\\STKI\\\\Minggu 11'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maria Sharapova has basically no friends as te...</td>\n",
       "      <td>https://www.tennisworldusa.org/tennis/news/Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BASEL, Switzerland (AP), Roger Federer advance...</td>\n",
       "      <td>http://www.tennis.com/pro-game/2018/10/copil-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Roger Federer has revealed that organisers of ...</td>\n",
       "      <td>https://scroll.in/field/899938/tennis-roger-fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kei Nishikori will try to end his long losing ...</td>\n",
       "      <td>http://www.tennis.com/pro-game/2018/10/nishiko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Federer, 37, first broke through on tour over ...</td>\n",
       "      <td>https://www.express.co.uk/sport/tennis/1036101...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                       article_text  \\\n",
       "0           1  Maria Sharapova has basically no friends as te...   \n",
       "1           2  BASEL, Switzerland (AP), Roger Federer advance...   \n",
       "2           3  Roger Federer has revealed that organisers of ...   \n",
       "3           4  Kei Nishikori will try to end his long losing ...   \n",
       "4           5  Federer, 37, first broke through on tour over ...   \n",
       "\n",
       "                                              source  \n",
       "0  https://www.tennisworldusa.org/tennis/news/Mar...  \n",
       "1  http://www.tennis.com/pro-game/2018/10/copil-s...  \n",
       "2  https://scroll.in/field/899938/tennis-roger-fe...  \n",
       "3  http://www.tennis.com/pro-game/2018/10/nishiko...  \n",
       "4  https://www.express.co.uk/sport/tennis/1036101...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/tennis_articles_v4.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## working with re ( regular expression in python)\n",
    "\n",
    "import re\n",
    "s = 'he&&&s'\n",
    "s = re.sub(\"[^a-zA-Z]\",\" \",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : Data Cleaning\n",
    "### Cleaning sentences, by removing Non Alphabet Characters and converting to Lower Case Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning sentences, by removing non alphabet characters and converting to lower case letters\n",
    "\n",
    "dict = {}\n",
    "s = \"\"\n",
    "for a in df['article_text']:\n",
    "      s += a\n",
    "# print s\n",
    "s = s.lower()\n",
    "# print s\n",
    "\n",
    "sentences = sent_tokenize(s)\n",
    "# print sentences\n",
    "\n",
    "final = []\n",
    "\n",
    "for s in sentences:\n",
    "      temp = re.sub(\"[^a-zA-Z]\",\" \",s)\n",
    "      temp = temp.lower()\n",
    "      final.append(temp)\n",
    "      dict[temp] = s\n",
    "# printfinal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : Building Senetnce Similarity Matrix\n",
    "### Similarity is found using Cosine Similarity between vector representation of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define method for calculating similarity\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : Sentence Ranking\n",
    "### Sentences are ranked using PageRank Algorithm on the Graph generated from the Sentence Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy) (2.3.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.3\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      "\n",
      "s p e a k i n g   a t   t h e   s w i s s   i n d o o r s   t o u r n a m e n t   w h e r e   h e   w i l l   p l a y   i n   s u n d a y s   f i n a l   a g a i n s t   r o m a n i a n   q u a l i f i e r   m a r i u s   c o p i l     t h e   w o r l d   n u m b e r   t h r e e   s a i d   t h a t   g i v e n   t h e   i m p o s s i b l y   s h o r t   t i m e   f r a m e   t o   m a k e   a   d e c i s i o n     h e   o p t e d   o u t   o f   a n y   c o m m i t m e n t  . t h e         t i m e   g r a n d   s l a m   c h a m p i o n   h a s   v o i c e d   d o u b t s   a b o u t   t h e   w i s d o m   o f   t h e   o n e   w e e k   f o r m a t   t o   b e   i n t r o d u c e d   b y   o r g a n i s e r s   k o s m o s     w h o   h a v e   p r o m i s e d   t h e   i n t e r n a t i o n a l   t e n n i s   f e d e r a t i o n   u p   t o         b i l l i o n   i n   p r i z e   m o n e y   o v e r   t h e   n e x t   q u a r t e r   c e n t u r y  . a r g e n t i n a   a n d   b r i t a i n   r e c e i v e d   w i l d   c a r d s   t o   t h e   n e w   l o o k   e v e n t     a n d   w i l l   c o m p e t e   a l o n g   w i t h   t h e   f o u r             s e m i   f i n a l i s t s   a n d   t h e         t e a m s   w h o   w i n   q u a l i f y i n g   r o u n d s   n e x t   f e b r u a r y  . w h e n   i   m   o n   t h e   c o u r t s   o r   w h e n   i   m   o n   t h e   c o u r t   p l a y i n g     i   m   a   c o m p e t i t o r   a n d   i   w a n t   t o   b e a t   e v e r y   s i n g l e   p e r s o n   w h e t h e r   t h e y   r e   i n   t h e   l o c k e r   r o o m   o r   a c r o s s   t h e   n e t   s o   i   m   n o t   t h e   o n e   t o   s t r i k e   u p   a   c o n v e r s a t i o n   a b o u t   t h e   w e a t h e r   a n d   k n o w   t h a t   i n   t h e   n e x t   f e w   m i n u t e s   i   h a v e   t o   g o   a n d   t r y   t o   w i n   a   t e n n i s   m a t c h  . b u t   w i t h   t h e   a t p   w o r l d   t o u r   f i n a l s   d u e   t o   b e g i n   n e x t   m o n t h     n a d a l   i s   r e a d y   t o   p r o v e   h i s   f i t n e s s   b e f o r e   t h e   s e a s o n   e n d i n g   e v e n t   a t   t h e         a r e n a  \n"
     ]
    }
   ],
   "source": [
    "# generating the final summary : graph is generated using networkx library \n",
    "# and cosine similarity matrix containing adjacency list;\n",
    "# after that sentences are scored using pagerank and sorted and stored in ranked_sentences\n",
    "\n",
    "# Step 2 - Generate Similarity Matrix across sentences\n",
    "stop_words = stopwords.words('english')\n",
    "sentence_similarity_matrix = build_similarity_matrix(final, stop_words)\n",
    "\n",
    "# Step 3 - Rank sentences in similarity matrix\n",
    "sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(\n",
    "    ((scores[i], s) for i, s in enumerate(final)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Step 5 - Output the summarized text\n",
    "summarize_text = []\n",
    "for i in range(5):   # ambil 5 kalimat teratas\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "print(\"Summarized Text:\\n\")\n",
    "print(\". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 : Summary Generation\n",
    "### Summary is outputted as the top 10 ranked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaking at the swiss indoors tournament where he will play in sundays final against romanian qualifier marius copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.\n",
      "the 20-time grand slam champion has voiced doubts about the wisdom of the one-week format to be introduced by organisers kosmos, who have promised the international tennis federation up to $3 billion in prize money over the next quarter-century.\n",
      "argentina and britain received wild cards to the new-look event, and will compete along with the four 2018 semi-finalists and the 12 teams who win qualifying rounds next february.\n",
      "when i'm on the courts or when i'm on the court playing, i'm a competitor and i want to beat every single person whether they're in the locker room or across the net.so i'm not the one to strike up a conversation about the weather and know that in the next few minutes i have to go and try to win a tennis match.\n",
      "but with the atp world tour finals due to begin next month, nadal is ready to prove his fitness before the season-ending event at the 02 arena.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "     print(dict[ranked_sentence[i][1]])\n",
    "        \n",
    "# for i in range(10):\n",
    "#       summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
